项目旨在构建一套融合计算机视觉与大语言模型的智能陪护系统，在前端设备本地识别老
年人高风险行为的基础上，引入行为节律偏离建模，实现对生活模式变化的趋势性监测；
同时通过语义生成模块提供温和自然的语言反馈，在不上传原始图像的前提下实现隐私友
好、可解释的人机交互。系统面向独居老人等重点群体，具备较强的社会关怀价值与落地
应用潜力。
本项目聚焦老龄社会背景下独居老人居家安全看护问题，综合计算机视觉（CV）、大语言模型（LLM
）与边缘部署等人工智能手段，构建一套“感知 + 端侧反馈 + 用户联动 + 社区协同”的智能陪护
系统。通过引入行为节律建模、风险偏离检测与语义反馈机制，实现隐私友好、可解释、可适配的老
年人居家看护技术方案。具体目标包括：
①行为识别目标：本项目采用部署在前端的多模态行为识别模型ActionCLIP（Wang et 
al.,arXiv:2109.08472）作为核心组件，构建轻量化的视频感知能力。ActionCLIP基于CLIP框架扩
展，采用双编码器结构分别对视频帧序列与自然语言提示进行建模，通过计算跨模态语义相似度实现
“视频-语言对齐”式的行为识别。该方法跳脱出传统固定类别分类模式，支持通过 prompt 动态注
入行为语义，具备零样本识别、类别灵活扩展等优势。项目将围绕“跌倒”“夜间起身”“异常弯腰
停留”等典型养老场景风险行为，设计专属提示模板并进行轻量级模型适配优化，实现高置信度的语
义标签输出。相较于纯视觉结构或时序结构，ActionCLIP在语义迁移能力、数据依赖度、边缘适配性
等方面具备显著优势，尤其适用于养老环境中行为模糊、标签不足的复杂场景。
② 节律建模目标：基于近7～30日的时序行为数据，构建个体化的生活行为节律模型（如起居规律、
如厕频率、活动分布等），监测长期趋势变化，并对偏离情况进行分级评估。
③ 智能反馈目标：通过结构化信息构建Prompt输入云端大语言模型，生成符合语境与身份（老人、
家属、社区）的个性化语义提示，实现人性化、高可接受度的语言反馈。
④ 社区协同目标：设计社区参与机制，实现风险信息与社区公共数据的联动响应，并引入社区标注
反馈机制反哺模型训练，构建“个体—家庭—社区”三位一体的联动照护模式。
2.2 项目研究主要内容
本项目以“行为建模—节律分析—反馈生成—协同响应”四阶段为主线，系统性展开老年人陪护场景
的智能识别、趋势建模与风险响应能力建设，具体内容如下：
（1） 数据采集与识别阶段
在居家场景部署 RGB 摄像头，基于轻量化适配后的多模态行为识别模型ActionCLIP（Wang et al.,
arXiv:2109.08472）构建端侧视频感知模块。该模型采用“视频-语言匹配”范式，通过
Transformer 编码器提取视频帧时序特征，并与自然语言 Prompt 表示进行语义对齐，支持对“跌倒
”“夜间起身”“异常久坐”等关键风险行为的精准识别。系统在前端完成全部推理过程，自动生成
结构化标签（如动作类型、起止时间、置信度等），并通过脱敏机制仅上传标签数据，原始视频默认
不上传。该机制既保障了居家识别过程的隐私安全，又具备良好的时效性与适配性，构建“隐私友好
+实时响应”的居家监护底座。
此外，本阶段将集成本地录像控制功能，支持用户自主设置录像保留策略（如仅记录高风险时段、模
糊处理保留等），增强系统的合规性与可调节性。
（2）节律建模与偏离检测阶段
基于长周期行为标签序列（7~30天），构建节律建模模块。通过滑动窗口提取个体行为在不同时间点
、不同行为类型上的出现频率和节奏特征，建立起“起居节律”
、
“如厕节律”
、
“夜间活动周期
”等个体化生活节律模型。
当天行为与节律基线进行比对，识别出行为偏离项，如：
•起居时间延迟或提前；
•异常如厕频率升高；
•活动时间段变化（如深夜久坐）等。
每个偏离项按幅度、频率与波动性进行量化打分，并归类为轻度/中度/严重风险，生成结构化风险列
表。后续可进一步引入节律可视化组件，形成趋势图谱与多维指标反馈，支持长期慢病干预与生活方
式调整建议。
（3）语义反馈与预警机制设计阶段
基于识别结果与节律偏移项，构建 Prompt 模板，引导大语言模型（如ChatGLM、GPT等）生成语义分
层的自然语言反馈。设计标准格式的 Prompt 输入结构，注入行为类型、偏离等级、行为上下文等关
键变量，引导模型区分语气风格与输出通道：
•用户提示（轻度）：温和提醒，如“昨晚入睡时间较平时延后，建议今晚提前休息”；
•家属通报（中度）：解释性提示，如“近期夜间如厕频率升高，建议关注饮水与泌尿情况”；
•社区通报（严重）：指令型语言，如“连续三日跌倒风险显著提升，建议介入健康评估”
。
此外，还将对 LLM 输出进行内容筛选与“反馈内容可控性调优”，确保语言不过度干涉、不制造恐
慌，真正实现“能解释、好接受”的AI陪护语言风格。
（4）社区协同机制建设阶段
设计社区作为“事件响应者”与“数据标注者”的双重角色通路。高风险事件发生后，系统将脱敏化
的风险摘要（如“凌晨2:30如厕三次+久坐1.5h”）发送给社区网格员或健康联络员，由其按风险等
级决定是否实地干预。处理结果（误报/已处理/未响应等）可通过 App 平台回传，作为事件标注样
本用于模型强化训练，实现“社区—AI系统”共训机制。
系统还支持联动社区环境数据（气温、疫情等），构建提示增强规则。例如“室温骤降+夜起频繁→
生成送温暖包建议”
，增强平台的场景感知力与人文智能表达力。
3.项目创新特色概述
本项目致力于构建“前端智能识别—节律趋势建模—可解释反馈—社区联动响应”的智能陪护系统
，系统性突破传统陪护系统“被动监测、单点反应、中心决策”的范式，在以下八个方面具有明显
创新性与先发优势：
（1）节律偏离分析机制
•传统不足：现有陪护系统多聚焦于单点行为（如跌倒、异常逗留），对用户长期行为节奏缺乏建模
能力，无法发现趋势性风险。
•项目创新：首次将老年人生活行为视为具备时序规律的节律数据，采用滑动窗口建模与行为分布分
析，识别行为偏移趋势并评估风险等级。
•实现效果：实现从“事件感知”向“趋势预测”的转型，为慢病预警、生活习惯干预、居家康养评
估提供技术支撑。
（2）可调节隐私友好型视觉感知设计
•传统不足：中心化系统需上传原始视频或长时段图像，存在数据泄露风险、合规隐患，用户对感知
系统缺乏信任。
•项目创新：采用“端测推理 + 结构化上传 + 本地控制”架构，所有图像仅用于本地一次性推理，
用户可自定义录像保留权限，云端仅接收行为标签与偏离结果。
•实现效果：构建隐私合规、透明可控的数据处理路径，提升用户接受度，保障系统部署的可持续性
与合规性。
（3）多模态辅助感知机制
•传统不足：视觉系统在夜间、遮挡、设备遮挡等情况下识别能力下降，鲁棒性有限。
•项目创新：引入低功耗辅助传感器（如声音触发、门磁状态、床压变化），构建多源协同机制，视
觉与非视觉信息互补支撑行为识别。
•实现效果：显著提升系统在复杂场景下的适应能力，扩大陪护场景覆盖范围，降低感知盲区出现率
。
（4）语义分级反馈机制
•传统不足：多数智能陪护系统反馈方式单一，缺乏语言语气控制，往往“冰冷提示”，无法提供人
性化关怀。
•项目创新：构建结构化Prompt模板，将偏离类型、风险等级、历史趋势等作为控制因子，引导大语
言模型生成分级语气的自然语言反馈。
•实现效果：输出语言具有“温度感”与“引导性”，区分普通建议、家庭通知、社区预警等多种语
义风格，提升AI系统的亲和力与可接受性。
（5）节律建模可视化接口
•传统不足：多数系统仅展示当前状态，不具备历史趋势反馈或可视化能力。
•项目创新：引入节律轨迹可视化组件，展示用户睡眠、活动、如厕等节律变化趋势，结合评分与偏
离项分析，构建行为健康画像。
•实现效果：帮助用户/家属主动掌握健康行为变化，为干预措施提供数据基础，同时提升系统可信
度。
（6）模型轻量优化与前端推理能力
•传统不足：主流多模态识别模型如 CLIP、VideoMAE 等通常参数量庞大，依赖强计算资源，难以直
接部署于 Jetson Nano、RK3588 等边缘设备，在养老陪护等嵌入式场景中部署成本高、运行稳定性
差。
•项目创新：本项目选用结构相对简洁、训练机制稳定的ActionCLIP架构作为基础模型，并结合边缘
计算特性对其进行剪枝压缩、Token减少与Prompt轻量化适配操作，使得模型整体推理负载控制在约
16.7 GFLOPS、显存占用控制在344MB左右，满足主流嵌入式平台运行需求。
•实现效果：在不牺牲识别性能的前提下，实现在低功耗边缘设备上的稳定部署，提升系统的可嵌入
性、部署灵活度与环境适应能力，为居家陪护系统的规模化推广提供算力基础支撑。
（7）社区协同响应与人机共训机制
•传统不足：多数陪护系统“家庭-社区”之间割裂，事件响应链不完整，系统缺乏反馈强化能力。
•项目创新：将社区作为系统“事件响应方 + 数据回馈方”，允许社区网格员基于系统报告判断干
预行为，并通过平台回传事件结果，作为后续模型训练样本。
•实现效果：构建社区与AI之间的数据闭环，实现人机共训，提升系统的实战演化能力，构建可持续
适应的陪护平台。
（8）政策响应性与社会融合导向
•对接战略：《中共中央 国务院关于加强基层治理体系和治理能力现代化建设的意见》提出：“推
动智能化服务手段融入社区治理体系”
。
•项目契合：本项目设计“老年个体—家庭成员—社区治理”三方闭环结构，将智能陪护嵌入社区治
理流程中，支持风险协同响应与公共服务联动。
•推广潜力：具备标准接口、社会角色可复制能力，可推广至各类街道、社区、智慧养老平台，形成
智能治理的“平台化模板”
。
4、项目研究技术路线
图 3 技术架构图
本项目技术路线围绕“前段视觉处理—节律建模—语义反馈—联动响应”四大核心逻辑，形成闭
环式智能陪护系统。具体路径如下：
第一步：居家行为数据采集与识别
部署居家 RGB 摄像头，在前端设备上运行轻量适配后ActionCLIP模型（Wang et al.,
arXiv:2109.08472）。该模型基于视频—语言匹配机制，采用双编码器结构分别对视频帧序列与自
然语言提示进行建模，通过跨模态语义相似度计算完成多类别行为的高精度识别。项目针对养老场景
下的关键行为（如跌倒、久坐、夜间异常活动等）设计特定 Prompt 模板，并构建行为识别类别适配
机制，支持以结构化形式输出行为标签、发生时段与置信度等核心指标。在确保图像不上传、仅上传
标签数据的基础上，系统完成前端隐私友好的初步行为感知流程，为后续节律建模与反馈响应提供语
义输入。第二步：节律建模与行为偏离检测
基于近7~30日的历史行为时序数据，构建个体生活节律模型，识别如起居、如厕、活动等行为的
时间规律。结合当前识别结果，进行偏离检测并打分，生成“轻度/中度/严重”等风险等级。
第三步：语义反馈生成与智能预警机制
将结构化行为信息与节律偏移项输入云端大语言模型，通过Prompt模板自动生成自然语言反馈语
句，并根据风险等级调整语气与反馈路径（老人语音提示/家属通知/社区通报），提升人机交互接受
度。
第四步：用户交互与社区联动响应
家属端可通过App接收反馈信息与视频摘要；社区可基于脱敏等级报告作出应对，并将处理结果
回传系统作为训练数据。最终实现“老年人—家庭—社区”三方协同的智能照护生态闭环。